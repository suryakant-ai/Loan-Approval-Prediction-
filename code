# importing libraries and datasets 
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt 
import seaborn as sns 
import matplotlib.pyplot as plt
data = pd.read_csv(r"C:\Users\91860\Desktop\Datasets and projects\LoanApprovalPrediction.csv")
data.head(5)
# Data pre-processing and visulaization
obj = (data.dtypes == 'object') 
print("Categorical variables:",len(list(obj[obj].index)))
bj = (data.dtypes == 'object') 
object_cols = list(obj[obj].index) 
plt.figure(figsize=(18,36)) 
index = 1

for col in object_cols: 
  y = data[col].value_counts() 
  plt.subplot(11,4,index) 
  plt.xticks(rotation=90) 
  sns.barplot(x=list(y.index), y=y) 
  index +=1
  plt.show()
  # for converting categorical data types into interger datatypes, we have to import lable encoder 
from sklearn import preprocessing 
LabelEncoder = preprocessing.LabelEncoder()
obj = (data.dtypes == 'object')
for col in list(obj[obj].index):
    data[col] = LabelEncoder.fit_transform(data[col])
    # To find the number of columns with data types == object
obj = (data.dtypes == 'object') 
print("Categorical variables:",len(list(obj[obj].index)))
# The corelation matrix with specified size and 
plt.figure(figsize=(12,6)) 

sns.heatmap(data.corr(),cmap='BrBG',fmt='.2f', 
            linewidths=2,annot=True)
            #The above heatmap is showing the correlation between Loan Amount and ApplicantIncome.
#It also shows that Credit_History has a high impact on Loan_Status.
#Now we will use Catplot to visualize the plot for the Gender, and Marital Status of the applicant.
sns.catplot(x="Gender", y="Married", 
            hue="Loan_Status",  
            kind="bar",  
            data=data)
            # Finding missing values in the data 
for col in data.columns: 
  data[col] = data[col].fillna(data[col].mean())  

data.isna().sum()
# as there is no missing values in our data, me proceed to futher splitting the data between training and testing 
from sklearn.model_selection import train_test_split 


X = data.drop(['Loan_Status'],axis=1) 
Y = data['Loan_Status'] 
X.shape,Y.shape 

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, 
                                                    test_size=0.4, 
                                                    random_state=1) 
X_train.shape, X_test.shape, Y_train.shape, Y_test.shape
#Model Training and Evaluation
#As this is a classification problem so we will be using these models : 
#KNeighborsClassifiers
#RandomForestClassifiers
#Support Vector Classifiers (SVC)
#Logistics Regression
#To predict the accuracy we will use the accuracy score function from scikit-learn library.from sklearn.neighbors import KNeighborsClassifier 
from sklearn.ensemble import RandomForestClassifier 
from sklearn.svm import SVC 
from sklearn.linear_model import LogisticRegression 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn import metrics 

knn = KNeighborsClassifier(n_neighbors=3) 
rfc = RandomForestClassifier(n_estimators = 7, 
							criterion = 'entropy', 
							random_state =7) 
svc = SVC() 
lc = LogisticRegression() 

# making predictions on the training set 
for clf in (rfc, knn, svc,lc): 
	clf.fit(X_train, Y_train) 
	Y_pred = clf.predict(X_train) 
	print("Accuracy score of ", 
		clf.__class__.__name__, 
		"=",100*metrics.accuracy_score(Y_train, 
										Y_pred))
          from sklearn.linear_model import LogisticRegression

# Create an instance of the LogisticRegression classifier with increased max_iter
log_reg = LogisticRegression(max_iter=200)  # Increase iterations to 200

# Fit the classifier on the training data
log_reg.fit(X_train, Y_train)

# Predict the target values for the test set
Y_pred = log_reg.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(Y_test, Y_pred)

print(f'Accuracy: {accuracy:.2f}')
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create an instance of the LogisticRegression classifier
log_reg = LogisticRegression(max_iter=100)  # Default iterations

# Fit the classifier on the scaled training data
log_reg.fit(X_train_scaled, Y_train)

# Predict the target values for the scaled test set
Y_pred = log_reg.predict(X_test_scaled)

# Calculate the accuracy of the model
accuracy = accuracy_score(Y_test, Y_pred)

print(f'Accuracy: {accuracy:.2f}')
from sklearn.linear_model import LogisticRegression

# Create an instance of the LogisticRegression classifier with an alternative solver
log_reg = LogisticRegression(solver='liblinear', max_iter=100)  # Using 'liblinear' solver

# Fit the classifier on the training data
log_reg.fit(X_train, Y_train)

# Predict the target values for the test set
Y_pred = log_reg.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(Y_test, Y_pred)

print(f'Accuracy: {accuracy:.2f}')

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Instantiate classifiers
models = {
    "Logistic Regression": LogisticRegression(),
    "KNN": KNeighborsClassifier(n_neighbors=3),
    "Random Forest": RandomForestClassifier(n_estimators=7, criterion='entropy', random_state=7),
    "SVM": SVC(probability=True)
}

# Train and evaluate models
for name, model in models.items():
    model.fit(X_train, Y_train)
    Y_train_pred = model.predict(X_train)
    Y_test_pred = model.predict(X_test)

    print(f"{name} Training Accuracy: {accuracy_score(Y_train, Y_train_pred):.2f}")
    print(f"{name} Test Accuracy: {accuracy_score(Y_test, Y_test_pred):.2f}")
    print(f"{name} Test Precision: {precision_score(Y_test, Y_test_pred):.2f}")
    print(f"{name} Test Recall: {recall_score(Y_test, Y_test_pred):.2f}")
    print(f"{name} Test F1 Score: {f1_score(Y_test, Y_test_pred):.2f}")
    print(f"{name} Test ROC AUC: {roc_auc_score(Y_test, model.predict_proba(X_test)[:,1]):.2f}\n")
    from sklearn.model_selection import GridSearchCV

# Hyperparameter tuning for Random Forest
param_grid = {
    'n_estimators': [10, 50, 100],
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 10, 20, 30]
}

grid_search = GridSearchCV(RandomForestClassifier(random_state=7), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, Y_train)

best_rf = grid_search.best_estimator_
Y_test_pred = best_rf.predict(X_test)
print(f"Best Random Forest Test Accuracy: {accuracy_score(Y_test, Y_test_pred):.2f}")
